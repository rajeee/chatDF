# Work Queue

Human-injected tasks. The loop checks this FIRST every iteration.
Add tasks as checkbox items. The loop will do the top unchecked one and mark it `[x]` when done.

## Tasks

- [x] **Remove correlation/statistics feature entirely**: The `compute_correlations` function uses `df.pearson_corr()` which no longer exists in Polars — broken in production. Delete: backend endpoint/worker code, frontend CorrelationMatrix component, "Show Correlations" button, related tests. Clean deletion. DO NOT PRUNE THIS TASK. *(Done in iteration 121 — removed all correlation code, endpoints, and tests.)*

- [x] **Fix ALL failing backend tests**: Run `pytest tests/ -q --ignore=tests/worker/test_timeout.py` and fix every failure. Known issues: (1) Compressed WS event types (`"cc"` not `"chat_complete"`) — update assertions. (2) `test_messages_table_structure` hardcoded column count — update. (3) Correlation tests — delete (removing feature). (4) `test_fetch.py` — update for CSV acceptance. (5) Rate limit tests expect 429 from async handlers — fix. ALL backend tests must pass after. DO NOT PRUNE THIS TASK. *(Done in iteration 122 — all 858 backend tests passing.)*

- [x] **Write Playwright E2E tests for core user journeys**: REAL end-to-end tests hitting actual backend (NO mocked API routes). Cover: (1) Paste dataset URL → schema appears. (2) Ask question → SQL results in DataGrid. (3) Visualize → chart renders. (4) Conversation CRUD. (5) Export CSV. Use dev-login for auth. Put in `implementation/frontend/tests/e2e/`. DO NOT PRUNE THIS TASK. *(Done in iteration 122 — 9 E2E test suites covering auth, chat flow, conversation CRUD, dataset loading, error recovery, etc.)*

- [x] **Harden LLM system prompt for Polars SQL**: (1) Add Polars SQL dialect notes — no ILIKE (use LOWER()+LIKE), no DATE_TRUNC (use strftime), string function differences. (2) Include 3-5 sample values per column in schema (modify `extract_schema` worker). (3) Add 3-4 few-shot query examples. (4) Improve `prune_context` to keep SQL result messages over plain text. (5) **Schema deduplication**: When multiple datasets are loaded, describe the first table's schema in full (columns, types, sample values), then for subsequent tables only list what DIFFERS — shared columns/types should be referenced as "same as table1.column_name". This saves significant context tokens when datasets have overlapping schemas. DO NOT PRUNE THIS TASK. *(Items 1-4 done in iterations 121-126. Item 5 (schema deduplication) done in iteration 129 — subsequent tables use "same as first_table.col" for matching columns.)*

- [x] **Cache query results in SQLite**: Cache query RESULTS (not downloaded files) in a `query_cache` SQLite table. Cache key = hash of the SQL with each table name expanded to its full dataset URL. Columns: `cache_key_hash TEXT PRIMARY KEY`, `result_json TEXT`, `column_names TEXT`, `row_count INTEGER`, `created_at TIMESTAMP`, `ttl_seconds INTEGER DEFAULT 3600`. On cache hit, skip the worker entirely — no re-download, no re-query. Add eviction for expired entries. DO NOT PRUNE THIS TASK. *(Done — two-tier cache: in-memory LRU (100 entries, 5min TTL) + persistent SQLite (500 entries, 1hr TTL) with SHA-256 key generation.)*

- [x] **Translate Polars SQL errors to user-friendly messages**: Add error translation layer mapping common Polars errors to helpful messages. "column X not found" → "Column 'X' doesn't exist. Available columns: ...". "ILIKE not supported" → explain LOWER()+LIKE. Show the translated message to the user, pass the raw error back to the LLM for retry. DO NOT PRUNE THIS TASK. *(Done — error_translator.py with 17 pattern categories, integrated into LLM chat flow.)*

- [x] **Enhanced token visibility in Dev Mode prompt inspector**: The pre-send prompt inspector modal (dev mode) should show a detailed token breakdown: (1) System prompt token count. (2) Each conversation turn listed separately with its token count (user message, assistant response, tool calls/results). (3) Tool declarations token count. (4) New user message token count. (5) Total tokens. Use a simple token estimator (chars/4 is fine, or tiktoken if available). Show as a stacked bar or itemized list so the user can see exactly what's consuming context. DO NOT PRUNE THIS TASK. *(Done in iteration 129 — stacked bar + itemized list with per-section token counts, collapsible breakdown.)*

